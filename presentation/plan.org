* Notes
** 1 min
*** Egor: intro
*** Egor: primary preprocessing
    + > 0.5
    + NLTK
** 2 min
*** Ilya: frequency bayesian
    + Use primary preproc
    + Chances of being toxic given word presence
*** Ilya: frequency formula
    + Impact
    + Scaling (rare words balanced)
** 2 min
*** Lev : RNN words
    + Use primary preproc
    + Glove 300 dimensions
*** Lev : RNN training
    + Failed to find a pre-fit model
    + Generally, a very non-trivial problem
** 1 min
*** Egor: Unbalanced data
    + Accuracy not applicable
    + Naive balance: drop most of non-toxic messages
    + Further work: rare class has a higher weight
** 1 min
*** Ilya: fit: frequency
*** Lev : fit: RNN
*** Ilya: cross-validation
    + CV
    + Grid search
    + unbalanced n = 3, weights uniform
** 3 min
*** Egor: final overview
    + Unbalanced: 3 RNNs and 3 freq
    + Balanced: only freq (RNNs take too long)
    + RNN has a higher potential (in the original context, LLMs were used)
    + RNNs: too much time for training, too few samples
    + Too few samples for adequate train test split and accuracy
    + But we only compare freq
    + Unbalanced data KNNs F_1 score is better, SVM â‰ˆ LDA
    + Balanced data LDA
